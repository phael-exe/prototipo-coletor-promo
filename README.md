# Desafio TÃ©cnico: Coletor de promoÃ§Ãµes do Mercado Livre

[![CI](https://github.com/phael-exe/prototipo-coletor-promo/actions/workflows/ci.yml/badge.svg)](https://github.com/phael-exe/prototipo-coletor-promo/actions/workflows/ci.yml)
[![Release](https://img.shields.io/github/v/release/phael-exe/prototipo-coletor-promo?include_prereleases)](https://github.com/phaelzin/prototipo-coletor-promo/releases)
[![Python](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## ğŸ“‹ VisÃ£o Geral

ProtÃ³tipo de um coletor de promoÃ§Ãµes do Mercado Livre que realiza:
- âœ… **Coleta** de produtos via web scraping com paginaÃ§Ã£o dinÃ¢mica
- âœ… **NormalizaÃ§Ã£o** dos dados em um modelo consistente (Pydantic)
- âœ… **PersistÃªncia** no BigQuery (Google Cloud)
- âœ… **DeduplicaÃ§Ã£o** para evitar registros repetidos

## ğŸ—ï¸ Arquitetura

```
prototipo-coletor-promo/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py          # ConfiguraÃ§Ãµes via variÃ¡veis de ambiente
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ product.py         # Schema Pydantic dos produtos
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ crawler.py         # ServiÃ§o de web scraping
â”‚       â””â”€â”€ bigquery.py        # ServiÃ§o de persistÃªncia BigQuery
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ crawler_teste.py       # Script de teste da coleta
â”‚   â””â”€â”€ bigquery_teste.py      # Script de teste completo (coleta + BigQuery)
â”œâ”€â”€ secrets/                   # Credenciais GCP (nÃ£o versionado)
â”œâ”€â”€ .env                       # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ .env.template              # Template das variÃ¡veis necessÃ¡rias
â”œâ”€â”€ .dockerignore              # ExclusÃµes para build Docker
â”œâ”€â”€ Dockerfile                 # Multi-stage build para Python 3.12
â”œâ”€â”€ docker-compose.yml         # OrquestraÃ§Ã£o de containers
â”œâ”€â”€ requirements.txt           # DependÃªncias Python
â”œâ”€â”€ README.md                  # Este arquivo
â”œâ”€â”€ CHANGELOG.md               # HistÃ³rico de mudanÃ§as
â””â”€â”€ LICENSE                    # LicenÃ§a MIT
```

### ğŸ“¦ Estrutura de Containers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         docker-compose.yml                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Service: collector                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Dockerfile (Multi-stage)            â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Stage 1 (Builder): gcc + deps     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Stage 2 (Runtime): app only       â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ Image: prototipo-coletor-promo      â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚         â†“                                   â”‚  â”‚
â”‚  â”‚  Volumes:                                   â”‚  â”‚
â”‚  â”‚  â€¢ ./secrets â†’ /app/secrets (ro)           â”‚  â”‚
â”‚  â”‚                                             â”‚  â”‚
â”‚  â”‚  Environment:                               â”‚  â”‚
â”‚  â”‚  â€¢ .env file (GCP credentials, API keys)   â”‚  â”‚
â”‚  â”‚  â€¢ PYTHONUNBUFFERED=1                      â”‚  â”‚
â”‚  â”‚  â€¢ GOOGLE_APPLICATION_CREDENTIALS          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Como Rodar Localmente

### 1. Clonar e configurar ambiente

```bash
git clone <repo-url>
cd prototipo-coletor-promo

# Criar ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou: .venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Configurar variÃ¡veis de ambiente

```bash
cp .env.template .env
# Editar .env com suas configuraÃ§Ãµes
```

VariÃ¡veis necessÃ¡rias:
```env
USER_AGENT="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36..."
GCP_PROJECT_ID="seu-projeto-gcp"
GCP_DATASET_ID="seu_dataset"
```

### 3. Configurar credenciais GCP

```bash
mkdir secrets
# Copie o arquivo JSON de credenciais do Service Account para:
# secrets/<seu-arquivo>.json
export GOOGLE_APPLICATION_CREDENTIALS="secrets/<seu-arquivo>.json"
```

### 4. Executar coleta completa

```bash
python scripts/bigquery_teste.py
```

---

## ğŸŒ API REST com FastAPI

A aplicaÃ§Ã£o possui uma API HTTP que permite disparar coletas programaticamente.

### Iniciar a API

```bash
# Via Docker Compose (recomendado)
docker compose up

# Ou localmente com uvicorn
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

A API estarÃ¡ disponÃ­vel em:
- **Base URL**: http://localhost:8000
- **Docs (Swagger)**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Endpoints DisponÃ­veis

#### `GET /` - Root
Retorna informaÃ§Ãµes bÃ¡sicas da API.

```bash
curl http://localhost:8000/
```

#### `GET /health` - Health Check
Verifica status da API e serviÃ§os dependentes (BigQuery).

```bash
curl http://localhost:8000/health
```

Resposta:
```json
{
  "status": "healthy",
  "timestamp": "2026-02-10T12:00:00Z",
  "version": "1.0.0",
  "services": {
    "crawler": "healthy",
    "bigquery": "healthy"
  }
}
```

#### `POST /collect` - Iniciar Coleta
Dispara uma coleta assÃ­ncrona de produtos.

```bash
curl -X POST http://localhost:8000/collect \
  -H "Content-Type: application/json" \
  -d '{
    "sources": ["monitor gamer 144hz", "ps5"],
    "limit_per_source": 50,
    "max_pages_per_source": 2,
    "delay_between_requests": 1.5,
    "persist_to_bigquery": true
  }'
```

Resposta (HTTP 202):
```json
{
  "task_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "execution_id": "a865239e",
  "status": "started",
  "message": "Coleta iniciada com sucesso. Use o task_id para consultar o resultado.",
  "sources": ["monitor gamer 144hz", "ps5"],
  "estimated_time_seconds": 30
}
```

**ParÃ¢metros:**

| Campo | Tipo | DescriÃ§Ã£o | PadrÃ£o | Limites |
|-------|------|-----------|--------|---------|
| `sources` | `List[str]` | Termos de busca | **obrigatÃ³rio** | min: 1 |
| `limit_per_source` | `int` | Produtos por fonte | `100` | 1-500 |
| `max_pages_per_source` | `int` | PÃ¡ginas por fonte | `3` | 1-10 |
| `delay_between_requests` | `float` | Delay em segundos | `1.5` | 0.5-5.0 |
| `persist_to_bigquery` | `bool` | Salvar no BigQuery | `true` | - |

#### `GET /collect/{task_id}` - Consultar Resultado
Retorna o resultado de uma coleta usando o `task_id`.

```bash
curl http://localhost:8000/collect/a1b2c3d4-e5f6-7890-abcd-ef1234567890
```

Resposta:
```json
{
  "execution_id": "a865239e",
  "status": "completed",
  "sources_processed": 2,
  "total_products_collected": 100,
  "products_inserted": 95,
  "products_duplicated": 5,
  "started_at": "2026-02-10T12:00:00Z",
  "completed_at": "2026-02-10T12:00:45Z",
  "error_message": null
}
```

**Status possÃ­veis:**
- `completed`: Coleta concluÃ­da com sucesso
- `failed`: Coleta falhou (veja `error_message`)

### Exemplos de Uso

**Python com requests:**
```python
import requests
import time

# 1. Inicia coleta
response = requests.post("http://localhost:8000/collect", json={
    "sources": ["monitor gamer", "teclado mecÃ¢nico"],
    "limit_per_source": 30,
    "persist_to_bigquery": True
})
data = response.json()
task_id = data["task_id"]
print(f"Coleta iniciada: {task_id}")

# 2. Aguarda e consulta resultado
time.sleep(data["estimated_time_seconds"] + 10)
result = requests.get(f"http://localhost:8000/collect/{task_id}").json()
print(f"Produtos coletados: {result['total_products_collected']}")
print(f"Inseridos no BQ: {result['products_inserted']}")
```

**cURL (linha de comando):**
```bash
# Health check
curl http://localhost:8000/health | jq

# Coleta simples
curl -X POST http://localhost:8000/collect \
  -H "Content-Type: application/json" \
  -d '{"sources": ["ps5"], "limit_per_source": 20}' | jq

# Consultar resultado (substitua TASK_ID)
curl http://localhost:8000/collect/TASK_ID | jq
```

#### DocumentaÃ§Ã£o Interativa (Swagger UI)

Acesse a documentaÃ§Ã£o automÃ¡tica no Swagger UI:

![Swagger UI - API Documentation](app/utils/img/image.png)

### Tratamento de Erros

A API retorna erros padronizados:

```json
{
  "error": "HTTPException",
  "message": "Task nÃ£o encontrada",
  "details": null,
  "timestamp": "2026-02-10T12:00:00Z"
}
```

**CÃ³digos HTTP:**
- `200`: Sucesso
- `202`: RequisiÃ§Ã£o aceita (coleta em andamento)
- `400`: ParÃ¢metros invÃ¡lidos
- `404`: Recurso nÃ£o encontrado
- `500`: Erro interno do servidor

---

## ğŸ³ Como Rodar com Docker

### 1. Build da imagem

```bash
# Build usando docker-compose
docker compose build

# Ou build manual com Docker
docker build -t prototipo-coletor-promo:latest .
```

### 2. Executar com Docker Compose

```bash
# Rodar uma vez e exibir logs
docker compose up

# Rodar em background
docker compose up -d

# Ver logs de execuÃ§Ã£o
docker compose logs -f collector

# Limpar containers e volumes
docker compose down
```

### 3. Executar com Docker direto

```bash
# Sem variÃ¡veis de ambiente
docker run --rm \
  -v ./secrets:/app/secrets:ro \
  -v ./.env:/app/.env:ro \
  prototipo-coletor-promo:latest

# Com variÃ¡veis de ambiente passadas explicitamente
docker run --rm \
  -v ./secrets:/app/secrets:ro \
  -e GCP_PROJECT_ID="seu-projeto-gcp" \
  -e GCP_DATASET_ID="seu_dataset" \
  -e GOOGLE_APPLICATION_CREDENTIALS="/app/secrets/gcp-credentials.json" \
  prototipo-coletor-promo:latest
```

### âš ï¸ Configurar credenciais GCP para Docker

A containerizaÃ§Ã£o requer que as credenciais GCP estejam acessÃ­veis. Existem duas abordagens:

**OpÃ§Ã£o A: Montar arquivo JSON (Desenvolvimento local)**

```bash
# Certifique-se de que as credenciais estÃ£o em ./secrets/
ls ./secrets/gcp-credentials.json

# Execute com volume mounted
docker compose up
```

**OpÃ§Ã£o B: Passar JSON como variÃ¡vel de ambiente (Cloud Run recomendado)**

1. Converta o arquivo JSON para variÃ¡vel de ambiente:
```bash
export GCP_CREDENTIALS_JSON=$(cat secrets/gcp-credentials.json | base64)
```

2. Modifique o `Dockerfile` (stage 2) para suportar:
```dockerfile
# No Dockerfile, apÃ³s ENV PYTHONUNBUFFERED=1
ARG GCP_CREDENTIALS_JSON
RUN if [ -n "$GCP_CREDENTIALS_JSON" ]; then \
      echo "$GCP_CREDENTIALS_JSON" | base64 -d > /app/secrets/credentials.json && \
      export GOOGLE_APPLICATION_CREDENTIALS=/app/secrets/credentials.json; \
    fi
```

3. Build e run:
```bash
docker build --build-arg GCP_CREDENTIALS_JSON="$GCP_CREDENTIALS_JSON" \
  -t prototipo-coletor-promo:latest .
```

### ğŸ” Verificar imagem Docker

```bash
# Ver tamanho da imagem
docker images prototipo-coletor-promo

# Inspecionar layers
docker inspect prototipo-coletor-promo:latest

# Ver logs do container
docker logs <container-id>
```

### ğŸ“‹ VariÃ¡veis de ambiente no Docker

O `docker-compose.yml` carrega automaticamente do arquivo `.env`:

```env
FIRECRAWL_API_KEY="sua-chave-aqui"
USER_AGENT="Mozilla/5.0 (X11; Linux x86_64)..."
GCP_PROJECT_ID="seu-projeto-gcp"
GCP_DATASET_ID="seu_dataset"
GOOGLE_APPLICATION_CREDENTIALS="secrets/gcp-credentials.json"
```

**Nota**: VariÃ¡veis sÃ£o sobrescritas pelo `docker-compose.yml` se conflitarem.

### ğŸš€ Deploy no Google Cloud Run

```bash
# 1. Configure gcloud CLI
gcloud auth login
gcloud config set project SEU-PROJETO-GCP

# 2. Build e push para Artifact Registry
gcloud builds submit --tag gcr.io/SEU-PROJETO/coletor-promo:latest

# 3. Deploy como Cloud Run Job
gcloud run jobs create coletor-promocoes \
  --image gcr.io/SEU-PROJETO/coletor-promo:latest \
  --region us-central1 \
  --memory 512Mi \
  --cpu 1 \
  --set-env-vars GCP_PROJECT_ID=SEU-PROJETO,GCP_DATASET_ID=seu_dataset \

# 4. Executar job
gcloud run jobs execute coletor-promocoes --region us-central1

# 5. Agendar execuÃ§Ã£o periÃ³dica com Cloud Scheduler
gcloud scheduler jobs create app-engine coletor-diario \
  --schedule="0 2 * * *" \
  --http-method=POST \
  --uri=https://SEU-REGION-SEU-PROJETO.cloudfunctions.net/trigger-job \
  --oidc-service-account-email=SEU-EMAIL@iam.gserviceaccount.com
```

---

## ğŸ“¦ MÃ³dulos Implementados

### ğŸ” Web Scraping (`app/services/crawler.py`)

ServiÃ§o de coleta que:
- Faz requisiÃ§Ãµes HTTP com User-Agent configurÃ¡vel
- Extrai dados do HTML usando BeautifulSoup
- Implementa retry com backoff exponencial (tenacity)
- Suporta **mÃºltiplas fontes** de busca simultÃ¢neas
- **PaginaÃ§Ã£o dinÃ¢mica** automÃ¡tica

**MÃ©todos principais:**
| MÃ©todo | DescriÃ§Ã£o |
|--------|-----------|
| `fetch_from_sources()` | Coleta de mÃºltiplas queries com paginaÃ§Ã£o |
| `fetch_products_paginated()` | Coleta paginada de uma query especÃ­fica |
| `fetch_products()` | Coleta simples (sem paginaÃ§Ã£o) |

**Campos extraÃ­dos:**
| Campo | DescriÃ§Ã£o |
|-------|-----------|
| `marketplace` | Identificador do marketplace (`mercado_livre`) |
| `item_id` | ID Ãºnico do produto (ex: `MLB12345678`) |
| `title` | TÃ­tulo do produto |
| `price` | PreÃ§o atual |
| `original_price` | PreÃ§o original (se houver desconto) |
| `discount_percent` | Percentual de desconto calculado |
| `seller` | Nome do vendedor |
| `url` | Link direto para o produto |
| `image_url` | URL da imagem principal |
| `source` | Query que gerou o item |
| `dedupe_key` | Chave Ãºnica para deduplicaÃ§Ã£o |
| `execution_id` | ID Ãºnico da execuÃ§Ã£o |
| `collected_at` | Timestamp da coleta |

---

### ğŸ—„ï¸ BigQuery (`app/services/bigquery.py`)

ServiÃ§o de persistÃªncia que:
- Cria tabela automaticamente se nÃ£o existir
- Insere dados via **LOAD JOB** (compatÃ­vel com free tier)
- Implementa **deduplicaÃ§Ã£o** antes da inserÃ§Ã£o
- Fornece estatÃ­sticas da tabela

**MÃ©todos principais:**
| MÃ©todo | DescriÃ§Ã£o |
|--------|-----------|
| `insert_products()` | Insere produtos com deduplicaÃ§Ã£o |
| `ensure_table_exists()` | Garante que a tabela existe |
| `get_stats()` | Retorna estatÃ­sticas da tabela |
| `get_recent_products()` | Busca produtos recentes |

---

## ï¿½ Fontes Coletadas

Atualmente, o sistema coleta dados do **Mercado Livre** via:

| Fonte | EstratÃ©gia | Status |
|-------|-----------|--------|
| **Mercado Livre** | Web scraping com paginaÃ§Ã£o dinÃ¢mica | âœ… Ativo |

### Query de Busca
- Aceita mÃºltiplas queries simultÃ¢neas
- Exemplo: "ps5", "monitor 144hz", "teclado mecÃ¢nico"
- Retorna produtos ordenados por relevÃ¢ncia
- Suporta paginaÃ§Ã£o automÃ¡tica (atÃ© 1000 itens por query)

---

## ğŸ”‘ EstratÃ©gia de DeduplicaÃ§Ã£o

A deduplicaÃ§Ã£o Ã© implementada usando a estratÃ©gia de **verificaÃ§Ã£o prÃ©-inserÃ§Ã£o** com `dedupe_key`.

### ComposiÃ§Ã£o da `dedupe_key`

```python
dedupe_key = f"{marketplace}_{item_id}_{price}"
```

Exemplo: `mercado_livre_MLB1234567_1299.90`

### Fluxo de DeduplicaÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Produto coletado                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Gera dedupe_key                    â”‚
â”‚  (marketplace + item_id + price)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Busca no BigQuery                  â”‚
â”‚  WHERE dedupe_key = ?               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  JÃ EXISTE? â†™â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  NÃƒO EXISTE? â†˜             â”‚         â”‚
â”‚              Descarta      â”‚         â”‚
â”‚              Insere        â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Vantagens desta Abordagem

| Aspecto | BenefÃ­cio |
|--------|----------|
| **Performance** | O(1) lookup com Ã­ndice |
| **PrecisÃ£o** | Detecta mesmo produto com preÃ§o diferente |
| **Simplicidade** | Sem necessidade de dados histÃ³ricos |
| **Flexibilidade** | Permite preÃ§o variar em nova coleta |

---

## ğŸ—„ï¸ Schema do BigQuery

### Tabela: `promotions`

```sql
CREATE TABLE IF NOT EXISTS `{project}.{dataset}.promotions` (
  -- IdentificaÃ§Ã£o
  marketplace STRING NOT NULL,           -- 'mercado_livre'
  item_id STRING NOT NULL,               -- 'MLB1234567890'
  dedupe_key STRING NOT NULL,            -- 'mercado_livre_MLB1234567_1299.90'
  
  -- InformaÃ§Ãµes do Produto
  title STRING NOT NULL,                 -- TÃ­tulo do produto
  price FLOAT64 NOT NULL,                -- PreÃ§o atual
  original_price FLOAT64,                -- PreÃ§o original (antes desconto)
  discount_percent FLOAT64,              -- % de desconto (calculado)
  seller STRING,                         -- Nome do vendedor
  
  -- Links e MÃ­dia
  url STRING NOT NULL,                   -- URL do produto
  image_url STRING,                      -- URL da imagem principal
  
  -- Contexto da Coleta
  source STRING NOT NULL,                -- Query que gerou (ex: 'ps5')
  execution_id STRING NOT NULL,          -- ID Ãºnico da execuÃ§Ã£o
  collected_at TIMESTAMP NOT NULL,       -- Quando foi coletado
  
  -- Metadata
  _inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()  -- Timestamp de inserÃ§Ã£o
);
```

### Ãndices Recomendados

```sql
-- Para deduplicaÃ§Ã£o rÃ¡pida
CREATE INDEX dedupe_idx ON `{project}.{dataset}.promotions` (dedupe_key);

-- Para buscas por data
CREATE INDEX date_idx ON `{project}.{dataset}.promotions` (DATE(collected_at));

-- Para anÃ¡lise por source
CREATE INDEX source_idx ON `{project}.{dataset}.promotions` (source);
```

### Query de ValidaÃ§Ã£o: Ãšltimas 24 Horas

```sql
-- Itens coletados nas Ãºltimas 24 horas
SELECT 
  DATETIME(CURRENT_TIMESTAMP(), 'America/Sao_Paulo') as momento_consulta,
  COUNT(*) as itens_coletados,
  COUNT(DISTINCT dedupe_key) as itens_unicos,
  COUNT(DISTINCT execution_id) as execucoes,
  COUNT(DISTINCT source) as queries_diferentes,
  ROUND(AVG(price), 2) as preco_medio,
  COUNTIF(discount_percent > 0) as itens_em_promocao,
  ROUND(AVG(discount_percent), 2) as desconto_medio
FROM `{project}.{dataset}.promotions`
WHERE DATE(collected_at, 'America/Sao_Paulo') = CURRENT_DATE('America/Sao_Paulo')
GROUP BY 1;
```

**Resultado esperado:**
```
momento_consulta              itens_coletados  itens_unicos  execucoes  ...
2026-02-10 15:30:45           1250             950           3          ...
```

---

## âš™ï¸ Trade-offs Implementados

### 1. **DeduplicaÃ§Ã£o PrÃ©-InserÃ§Ã£o vs PÃ³s-InserÃ§Ã£o**

| Escolha | RazÃ£o |
|---------|-------|
| âœ… **PrÃ©-InserÃ§Ã£o (implementado)** | Economiza quota BigQuery (free tier) |
| âŒ PÃ³s-InserÃ§Ã£o | Consome mais quota e INSERT/UPDATE |

### 2. **Compartilhar JSON Logger vs Logger PadrÃ£o**

| Escolha | RazÃ£o |
|---------|-------|
| âœ… **JSON Logger (implementado)** | CompatÃ­vel com Cloud Logging |
| âŒ Logger padrÃ£o | DifÃ­cil de parsear em produÃ§Ã£o |

### 3. **Cloud Run Service vs Cloud Run Job**

| Escolha | RazÃ£o |
|---------|-------|
| âœ… **Cloud Run Service (implementado)** | Sempre disponÃ­vel via HTTP, melhor observabilidade |
| âŒ Cloud Run Job | Mais caro para execuÃ§Ãµes frequentes |

### 4. **LOAD JOB vs INSERT Statements**

| Escolha | RazÃ£o |
|---------|-------|
| âœ… **LOAD JOB (implementado)** | 1 job = muitas linhas, compatÃ­vel com free tier |
| âŒ INSERT Statements | N jobs = N inserts, consome quota rÃ¡pido |

### 5. **Single Execution ID vs Multiple**

| Escolha | RazÃ£o |
|---------|-------|
| âœ… **Single per request (implementado)** | Agrupa coletas relacionadas, rastreabilidade |
| âŒ Multiple per product | DifÃ­cil correlacionar erros |

---

## ğŸ“ Diagrama da Arquitetura

```mermaid
graph TB
    User["ğŸ‘¤ UsuÃ¡rio"]
    GitHub["ğŸ™ GitHub"]
    API["ğŸš€ FastAPI Server"]
    Crawler["ğŸ•·ï¸ Web Scraper"]
    ML["ğŸ›’ Mercado Livre"]
    GCS["â˜ï¸ Google Cloud Storage"]
    BQ["ğŸ“Š BigQuery"]
    CloudRun["â˜ï¸ Cloud Run"]
    CloudLogging["ğŸ“‹ Cloud Logging"]
    
    User -->|1. POST /collect| API
    API -->|2. Start Background Task| Crawler
    Crawler -->|3. Fetch + Parse| ML
    Crawler -->|4. Dedupe Check| BQ
    Crawler -->|5. Load Job| GCS
    GCS -->|6. Bulk Insert| BQ
    API -->|7. Return task_id| User
    API -->|8. Logs em JSON| CloudLogging
    CloudLogging -->|9. Visualiza| User
    
    GitHub -->|Auto Deploy| CloudRun
    CloudRun -->|Executa| API
    
    style API fill:#4285f4,color:#fff
    style BQ fill:#f59e0b,color:#fff
    style CloudRun fill:#4285f4,color:#fff
    style Crawler fill:#10b981,color:#fff
    style CloudLogging fill:#8b5cf6,color:#fff
```

### Fluxo Detalhado

```
1. REQUISIÃ‡ÃƒO
   POST /collect { sources: ["ps5"], limit_per_source: 50 }
   â†“
2. ACEITAÃ‡ÃƒO
   202 Accepted { task_id, estimated_time }
   â†“
3. PROCESSAMENTO (Background)
   For each source:
     â†’ Fetch paginated results from Mercado Livre
     â†’ Parse HTML with BeautifulSoup
     â†’ Create Product models
     â†’ Check dedupe_keys in BigQuery
     â†’ Filter duplicates
   â†“
4. PERSISTÃŠNCIA
   â†’ Stage deduplicated products to GCS
   â†’ Load to BigQuery via LOAD JOB
   â†“
5. RESULTADO
   GET /collect/{task_id}
   â† { status: completed, inserted: 45, duplicated: 5 }
   â†“
6. OBSERVABILIDADE
   â†’ All logs to Cloud Logging as JSON
   â†’ Metrics available in BigQuery
```

### Fluxo de DeduplicaÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Produtos       â”‚
â”‚  coletados      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Extrai dedupe_keys dos produtos â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Consulta BigQuery:              â”‚
â”‚     SELECT DISTINCT dedupe_key      â”‚
â”‚     FROM promotions                 â”‚
â”‚     WHERE dedupe_key IN (...)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Filtra produtos novos           â”‚
â”‚     (dedupe_key nÃ£o existe)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Insere apenas produtos novos    â”‚
â”‚     via LOAD JOB                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Por que essa estratÃ©gia?

| Vantagem | DescriÃ§Ã£o |
|----------|-----------|
| **Simplicidade** | NÃ£o requer MERGE ou stored procedures |
| **Performance** | Query de verificaÃ§Ã£o Ã© rÃ¡pida com IN clause |
| **Atomicidade** | Cada inserÃ§Ã£o Ã© independente |
| **Free Tier** | Funciona sem streaming insert |
| **IdempotÃªncia** | Rodar mÃºltiplas vezes nÃ£o duplica dados |

### ImplementaÃ§Ã£o no cÃ³digo

```python
# app/services/bigquery.py

def insert_products(self, products: List[ProductSchema]) -> dict:
    # 1. Busca dedupe_keys existentes
    existing_keys = self._get_existing_dedupe_keys([p.dedupe_key for p in products])
    
    # 2. Filtra apenas produtos novos
    new_products = [p for p in products if p.dedupe_key not in existing_keys]
    duplicates = len(products) - len(new_products)
    
    # 3. Insere apenas os novos
    # ... (LOAD JOB com NDJSON)
```

### Resultado em execuÃ§Ã£o

```
ğŸ“Š RESULTADO DA INSERÃ‡ÃƒO:
   âœ… Inseridos:   292
   â­ï¸  Duplicados:  8
   âŒ Erros:       0
```

---

## ğŸ”§ ConfiguraÃ§Ãµes (`app/core/config.py`)

| VariÃ¡vel | DescriÃ§Ã£o | Default |
|----------|-----------|---------|
| `USER_AGENT` | User-Agent para requisiÃ§Ãµes | Chrome Linux |
| `MAX_RETRIES` | Tentativas mÃ¡ximas em caso de falha | 3 |
| `RETRY_MIN_SECONDS` | Tempo mÃ­nimo entre retries | 2 |
| `RETRY_MAX_SECONDS` | Tempo mÃ¡ximo entre retries | 10 |
| `GCP_PROJECT_ID` | ID do projeto GCP | - |
| `GCP_DATASET_ID` | ID do dataset BigQuery | - |

---

## ğŸ“Š Exemplo de ExecuÃ§Ã£o

```bash
$ python scripts/bigquery_teste.py

======================================================================
ğŸ—„ï¸  COLETA MULTI-FONTE COM PAGINAÃ‡ÃƒO + BIGQUERY
======================================================================

ğŸ“‹ CONFIGURAÃ‡ÃƒO:
   Fontes: ['monitor gamer 144hz', 'iphone 16', 'ps5']
   Limite por fonte: 100
   MÃ¡x. pÃ¡ginas por fonte: 3

ğŸ“¦ monitor gamer 144hz
   Produtos coletados: 100
   Em promoÃ§Ã£o: 22
   PreÃ§o mÃ©dio: R$ 1383.64

ğŸ“¦ iphone 16
   Produtos coletados: 100
   Em promoÃ§Ã£o: 45
   PreÃ§o mÃ©dio: R$ 6862.39

ğŸ“¦ ps5
   Produtos coletados: 100
   Em promoÃ§Ã£o: 17
   PreÃ§o mÃ©dio: R$ 5087.34

ğŸ¯ TOTAL COLETADO: 300 produtos

ğŸ“Š RESULTADO DA INSERÃ‡ÃƒO:
   âœ… Inseridos:   292
   â­ï¸  Duplicados:  8
   âŒ Erros:       0

ğŸ“ˆ ESTATÃSTICAS DO BIGQUERY:
   Total de produtos:    302
   Itens Ãºnicos:         298
   Produtos em promoÃ§Ã£o: 84
```

---

## ğŸ“ PrÃ³ximos Passos

- [x] Web scraping com requests + BeautifulSoup
- [x] NormalizaÃ§Ã£o completa (todos os campos do desafio)
- [x] PersistÃªncia no BigQuery
- [x] DeduplicaÃ§Ã£o por `dedupe_key`
- [x] Coleta multi-fonte com paginaÃ§Ã£o
- [x] Dockerfile e docker-compose
- [x] API FastAPI com endpoints `/health` e `/collect`
- [ ] Deploy no Cloud Run (GCP)
- [ ] Logs estruturados (JSON)
- [ ] AutenticaÃ§Ã£o/Rate limiting na API

---

## ğŸ› ï¸ Tecnologias Utilizadas

| Tecnologia | Uso |
|------------|-----|
| Python 3.12 | Linguagem principal |
| FastAPI | Framework web assÃ­ncrono |
| Uvicorn | Servidor ASGI de alta performance |
| requests | RequisiÃ§Ãµes HTTP |
| BeautifulSoup4 | Parsing de HTML |
| Pydantic | ValidaÃ§Ã£o e schemas |
| tenacity | Retry com backoff |
| google-cloud-bigquery | PersistÃªncia no BigQuery |
| python-json-logger | Logs estruturados em JSON |
| Docker | ContainerizaÃ§Ã£o |

---

## ğŸ” Logs Estruturados em JSON

O sistema implementa logs estruturados compatÃ­veis com Cloud Logging:

### ConfiguraÃ§Ã£o

```python
from app.core.logging import configure_logging, get_logger

# Inicializar
configure_logging(level="INFO")
logger = get_logger(__name__)

# Usar com contexto
logger.info("Collection started", extra={
    "task_id": task_id,
    "execution_id": execution_id,
    "sources": sources
})
```

### Campos PadrÃ£o em JSON

```json
{
  "timestamp": "2026-02-10T23:16:02.075760+00:00",
  "level": "info",
  "module": "app.routes.collect",
  "message": "Collection task completed",
  "task_id": "31bf705c-4f78-4048-a074-3d46180fe9cf",
  "execution_id": "59a50e2c",
  "duration_seconds": 32.38
}
```

Todos os logs sÃ£o compatÃ­veis com:
- âœ… GCP Cloud Logging
- âœ… AWS CloudWatch
- âœ… Datadog
- âœ… ELK Stack

Para mais detalhes, veja [LOGGING_GUIDE.md](LOGGING_GUIDE.md)

---

## ğŸš€ CI/CD: GitHub Actions + Cloud Run

### Fluxo AutomÃ¡tico de Deploy

```
GitHub Release
    â†“
Deploy Workflow
    â†“
Build Docker Image
    â†“
Push Artifact Registry
    â†“
Deploy Cloud Run
    â†“
Health Check + Cloud Logging Setup
```

### Setup Inicial

#### 1. Criar Service Account no GCP

```bash
export GCP_PROJECT_ID="seu-projeto-gcp"
export SERVICE_ACCOUNT_NAME="github-actions-deployer"

# Criar Service Account
gcloud iam service-accounts create ${SERVICE_ACCOUNT_NAME} \
  --project=${GCP_PROJECT_ID}

# Adicionar permissÃµes
gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/artifactregistry.admin"

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/run.admin"

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/iam.serviceAccountUser"

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/logging.admin"

gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \
  --member="serviceAccount:${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/bigquery.admin"
```

#### 2. Criar Artifact Registry

```bash
gcloud artifacts repositories create docker-repo \
  --repository-format=docker \
  --location=us-central1 \
  --project=${GCP_PROJECT_ID}
```

#### 3. Adicionar Secrets no GitHub

```bash
# Gerar chave JSON
gcloud iam service-accounts keys create key.json \
  --iam-account=${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com

# Adicionar secrets
gh secret set GCP_PROJECT_ID --body "seu-projeto-gcp"
gh secret set GCP_REGION --body "us-central1"
gh secret set GCP_DATASET_ID --body "promocoes_teste"
gh secret set GCP_SA_KEY --body "$(cat key.json)"
gh secret set GCP_CLOUD_RUN_SA --body "cloud-run-app@seu-projeto-gcp.iam.gserviceaccount.com"

rm key.json
```

### Fazer Deploy

**OpÃ§Ã£o 1: Release (AutomÃ¡tico)**
```bash
git tag v1.3.0
git push origin v1.3.0
```

**OpÃ§Ã£o 2: Manual (workflow_dispatch)**
```bash
gh workflow run deploy.yml -f environment=staging
```

O workflow farÃ¡ automaticamente:
- âœ… Build da imagem Docker
- âœ… Push para Artifact Registry
- âœ… Deploy no Cloud Run
- âœ… Health check
- âœ… ConfiguraÃ§Ã£o de Cloud Logging

---

## ğŸ“Š BigQuery: Queries Ãšteis

### 1. Total de Produtos

```sql
SELECT 
  COUNT(*) as total_products,
  COUNT(DISTINCT dedupe_key) as unique_products,
  COUNT(DISTINCT execution_id) as total_collections,
  ROUND(AVG(price), 2) as avg_price
FROM `{project}.{dataset}.promotions`
```

### 2. Produtos por Fonte

```sql
SELECT 
  source,
  COUNT(*) as total,
  COUNT(DISTINCT dedupe_key) as unique_items,
  ROUND(AVG(price), 2) as avg_price,
  COUNTIF(discount_percent > 0) as on_sale
FROM `{project}.{dataset}.promotions`
GROUP BY source
ORDER BY total DESC
```

### 3. Descontos Mais Altos

```sql
SELECT 
  title,
  source,
  price,
  original_price,
  discount_percent,
  url
FROM `{project}.{dataset}.promotions`
WHERE discount_percent IS NOT NULL
ORDER BY discount_percent DESC
LIMIT 20
```

### 4. EvoluÃ§Ã£o de Coletas por Dia

```sql
SELECT 
  DATE(collected_at) as data,
  COUNT(*) as produtos,
  COUNT(DISTINCT execution_id) as coletas,
  ROUND(AVG(price), 2) as preco_medio
FROM `{project}.{dataset}.promotions`
GROUP BY data
ORDER BY data DESC
```

### 5. HistÃ³rico de PreÃ§os de um Produto

```sql
SELECT 
  title,
  collected_at,
  price,
  discount_percent,
  LAG(price) OVER (ORDER BY collected_at) as preco_anterior,
  ROUND((price - LAG(price) OVER (ORDER BY collected_at)) / 
        LAG(price) OVER (ORDER BY collected_at) * 100, 2) as variacao_pct
FROM `{project}.{dataset}.promotions`
WHERE LOWER(title) LIKE '%ps5%'
ORDER BY collected_at DESC
```

### 6. Produtos Mais Buscados

```sql
SELECT 
  source,
  COUNT(*) as vezes_encontrado,
  ROUND(AVG(price), 2) as preco_medio,
  COUNTIF(discount_percent > 0) as com_desconto
FROM `{project}.{dataset}.promotions`
GROUP BY source
ORDER BY vezes_encontrado DESC
LIMIT 20
```

**Substitua:**
- `{project}` â†’ Seu GCP Project ID
- `{dataset}` â†’ Seu Dataset BigQuery

Para mais queries, veja [BIGQUERY_QUERIES.md](BIGQUERY_QUERIES.md)

---

## ğŸ” Monitoramento

### Cloud Logging

Acesse os logs estruturados:
```
https://console.cloud.google.com/logs/query?project=seu-projeto-gcp
```

**Filtros Ãºteis:**
```
# Todos os logs da aplicaÃ§Ã£o
resource.type="cloud_run_revision"
jsonPayload.module=~"app.*"

# Apenas erros
resource.type="cloud_run_revision"
jsonPayload.level="error"

# Logs de uma task especÃ­fica
resource.type="cloud_run_revision"
jsonPayload.task_id="sua-task-id"
```

### Cloud Run Dashboard

```
https://console.cloud.google.com/run?project=seu-projeto-gcp
```

Visualize:
- Status das deployments
- Performance metrics
- Erros e logs
- Traffic patterns

### BigQuery

```
https://console.cloud.google.com/bigquery?project=seu-projeto-gcp
```

Analise os dados coletados em tempo real

---

## ğŸ“œ Changelog

### v1.2.0 - Logs Estruturados em JSON (2026-02-10)

Release com migraÃ§Ã£o completa para logs estruturados:

- **Logs em JSON**: Estrutura padrÃ£o com timestamp, level, module
- **python-json-logger**: IntegraÃ§Ã£o com Cloud Logging
- **Contexto completo**: execution_id, task_id em cada log
- **RemoÃ§Ã£o de prints**: Todos substituÃ­dos por logger calls
- **Cloud Logging ready**: Pronto para GCP

Para detalhes, veja [CHANGELOG.md](CHANGELOG.md)

### v1.1.0 - API REST com FastAPI (2026-02-10)

Release focada em transformar a aplicaÃ§Ã£o em serviÃ§o web:

- **API REST**: 4 endpoints (GET /, GET /health, POST /collect, GET /collect/{task_id})
- **DocumentaÃ§Ã£o automÃ¡tica**: Swagger UI + ReDoc
- **Coleta assÃ­ncrona**: BackgroundTasks do FastAPI
- **Schemas validados**: Pydantic models para requests/responses
- **Tratamento de erros**: JSON standardizado

### v1.0.0 - Primeira Release (2026-02-09) ğŸ‰

MVP completo do coletor de promoÃ§Ãµes com todas as funcionalidades core:

- **Coleta Multi-Fonte**: suporte a mÃºltiplas queries simultÃ¢neas com paginaÃ§Ã£o dinÃ¢mica
- **PersistÃªncia BigQuery**: integraÃ§Ã£o completa com Google BigQuery via LOAD JOB
- **DeduplicaÃ§Ã£o**: verificaÃ§Ã£o prÃ©-inserÃ§Ã£o com `dedupe_key` (marketplace + item_id + price)
- **NormalizaÃ§Ã£o**: schema Pydantic com todos os campos do desafio
- **CI/CD**: GitHub Actions para lint, validaÃ§Ã£o e releases automÃ¡ticas

ğŸ“Š **MÃ©tricas de teste**: 300 produtos coletados, 292 inseridos, 8 duplicatas ignoradas

---

## ğŸ“œ Versionamento

Este projeto usa [Semantic Versioning](https://semver.org/). 
Veja o [CHANGELOG.md](CHANGELOG.md) para histÃ³rico completo de mudanÃ§as.

### Como criar uma release

```bash
# Commit suas mudanÃ§as
git add .
git commit -m "feat: nova funcionalidade"

# Crie e push a tag
git tag -a v1.1.0 -m "Release v1.1.0 - Nova Feature"
git push origin v1.1.0
```

O GitHub Actions criarÃ¡ automaticamente a release com os artefatos.

---

> Desenvolvido para o processo seletivo PromoZone