# Desafio TÃ©cnico: Coletor de promoÃ§Ãµes do Mercado Livre

[![CI](https://github.com/phaelzin/prototipo-coletor-promo/actions/workflows/ci.yml/badge.svg)](https://github.com/phaelzin/prototipo-coletor-promo/actions/workflows/ci.yml)
[![Release](https://img.shields.io/github/v/release/phaelzin/prototipo-coletor-promo?include_prereleases)](https://github.com/phaelzin/prototipo-coletor-promo/releases)
[![Python](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## ğŸ“‹ VisÃ£o Geral

ProtÃ³tipo de um coletor de promoÃ§Ãµes do Mercado Livre que realiza:
- âœ… **Coleta** de produtos via web scraping com paginaÃ§Ã£o dinÃ¢mica
- âœ… **NormalizaÃ§Ã£o** dos dados em um modelo consistente (Pydantic)
- âœ… **PersistÃªncia** no BigQuery (Google Cloud)
- âœ… **DeduplicaÃ§Ã£o** para evitar registros repetidos

## ğŸ—ï¸ Arquitetura

```
prototipo-coletor-promo/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py          # ConfiguraÃ§Ãµes via variÃ¡veis de ambiente
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ product.py         # Schema Pydantic dos produtos
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ crawler.py         # ServiÃ§o de web scraping
â”‚       â””â”€â”€ bigquery.py        # ServiÃ§o de persistÃªncia BigQuery
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ crawler_teste.py       # Script de teste da coleta
â”‚   â””â”€â”€ bigquery_teste.py      # Script de teste completo (coleta + BigQuery)
â”œâ”€â”€ secrets/                   # Credenciais GCP (nÃ£o versionado)
â”œâ”€â”€ .env                       # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ .env.template              # Template das variÃ¡veis necessÃ¡rias
â””â”€â”€ requirements.txt           # DependÃªncias Python
```

## ğŸš€ Como Rodar Localmente

### 1. Clonar e configurar ambiente

```bash
git clone <repo-url>
cd prototipo-coletor-promo

# Criar ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou: .venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Configurar variÃ¡veis de ambiente

```bash
cp .env.template .env
# Editar .env com suas configuraÃ§Ãµes
```

VariÃ¡veis necessÃ¡rias:
```env
USER_AGENT="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36..."
GCP_PROJECT_ID="seu-projeto-gcp"
GCP_DATASET_ID="seu_dataset"
```

### 3. Configurar credenciais GCP

```bash
mkdir secrets
# Copie o arquivo JSON de credenciais do Service Account para:
# secrets/<seu-arquivo>.json
export GOOGLE_APPLICATION_CREDENTIALS="secrets/<seu-arquivo>.json"
```

### 4. Executar coleta completa

```bash
python scripts/bigquery_teste.py
```

---

## ğŸ“¦ MÃ³dulos Implementados

### ğŸ” Web Scraping (`app/services/crawler.py`)

ServiÃ§o de coleta que:
- Faz requisiÃ§Ãµes HTTP com User-Agent configurÃ¡vel
- Extrai dados do HTML usando BeautifulSoup
- Implementa retry com backoff exponencial (tenacity)
- Suporta **mÃºltiplas fontes** de busca simultÃ¢neas
- **PaginaÃ§Ã£o dinÃ¢mica** automÃ¡tica

**MÃ©todos principais:**
| MÃ©todo | DescriÃ§Ã£o |
|--------|-----------|
| `fetch_from_sources()` | Coleta de mÃºltiplas queries com paginaÃ§Ã£o |
| `fetch_products_paginated()` | Coleta paginada de uma query especÃ­fica |
| `fetch_products()` | Coleta simples (sem paginaÃ§Ã£o) |

**Campos extraÃ­dos:**
| Campo | DescriÃ§Ã£o |
|-------|-----------|
| `marketplace` | Identificador do marketplace (`mercado_livre`) |
| `item_id` | ID Ãºnico do produto (ex: `MLB12345678`) |
| `title` | TÃ­tulo do produto |
| `price` | PreÃ§o atual |
| `original_price` | PreÃ§o original (se houver desconto) |
| `discount_percent` | Percentual de desconto calculado |
| `seller` | Nome do vendedor |
| `url` | Link direto para o produto |
| `image_url` | URL da imagem principal |
| `source` | Query que gerou o item |
| `dedupe_key` | Chave Ãºnica para deduplicaÃ§Ã£o |
| `execution_id` | ID Ãºnico da execuÃ§Ã£o |
| `collected_at` | Timestamp da coleta |

---

### ğŸ—„ï¸ BigQuery (`app/services/bigquery.py`)

ServiÃ§o de persistÃªncia que:
- Cria tabela automaticamente se nÃ£o existir
- Insere dados via **LOAD JOB** (compatÃ­vel com free tier)
- Implementa **deduplicaÃ§Ã£o** antes da inserÃ§Ã£o
- Fornece estatÃ­sticas da tabela

**MÃ©todos principais:**
| MÃ©todo | DescriÃ§Ã£o |
|--------|-----------|
| `insert_products()` | Insere produtos com deduplicaÃ§Ã£o |
| `ensure_table_exists()` | Garante que a tabela existe |
| `get_stats()` | Retorna estatÃ­sticas da tabela |
| `get_recent_products()` | Busca produtos recentes |

---

## ğŸ”‘ EstratÃ©gia de DeduplicaÃ§Ã£o

A deduplicaÃ§Ã£o Ã© implementada usando a estratÃ©gia de **verificaÃ§Ã£o prÃ©-inserÃ§Ã£o** com `dedupe_key`.

### ComposiÃ§Ã£o da `dedupe_key`

```
dedupe_key = f"{marketplace}_{item_id}_{price}"
```

Exemplo: `mercado_livre_MLB1234567_1299.90`

### Fluxo de DeduplicaÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Produtos       â”‚
â”‚  coletados      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Extrai dedupe_keys dos produtos â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Consulta BigQuery:              â”‚
â”‚     SELECT DISTINCT dedupe_key      â”‚
â”‚     FROM promotions                 â”‚
â”‚     WHERE dedupe_key IN (...)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Filtra produtos novos           â”‚
â”‚     (dedupe_key nÃ£o existe)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Insere apenas produtos novos    â”‚
â”‚     via LOAD JOB                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Por que essa estratÃ©gia?

| Vantagem | DescriÃ§Ã£o |
|----------|-----------|
| **Simplicidade** | NÃ£o requer MERGE ou stored procedures |
| **Performance** | Query de verificaÃ§Ã£o Ã© rÃ¡pida com IN clause |
| **Atomicidade** | Cada inserÃ§Ã£o Ã© independente |
| **Free Tier** | Funciona sem streaming insert |
| **IdempotÃªncia** | Rodar mÃºltiplas vezes nÃ£o duplica dados |

### ImplementaÃ§Ã£o no cÃ³digo

```python
# app/services/bigquery.py

def insert_products(self, products: List[ProductSchema]) -> dict:
    # 1. Busca dedupe_keys existentes
    existing_keys = self._get_existing_dedupe_keys([p.dedupe_key for p in products])
    
    # 2. Filtra apenas produtos novos
    new_products = [p for p in products if p.dedupe_key not in existing_keys]
    duplicates = len(products) - len(new_products)
    
    # 3. Insere apenas os novos
    # ... (LOAD JOB com NDJSON)
```

### Resultado em execuÃ§Ã£o

```
ğŸ“Š RESULTADO DA INSERÃ‡ÃƒO:
   âœ… Inseridos:   292
   â­ï¸  Duplicados:  8
   âŒ Erros:       0
```

---

## ğŸ”§ ConfiguraÃ§Ãµes (`app/core/config.py`)

| VariÃ¡vel | DescriÃ§Ã£o | Default |
|----------|-----------|---------|
| `USER_AGENT` | User-Agent para requisiÃ§Ãµes | Chrome Linux |
| `MAX_RETRIES` | Tentativas mÃ¡ximas em caso de falha | 3 |
| `RETRY_MIN_SECONDS` | Tempo mÃ­nimo entre retries | 2 |
| `RETRY_MAX_SECONDS` | Tempo mÃ¡ximo entre retries | 10 |
| `GCP_PROJECT_ID` | ID do projeto GCP | `promozone-ml` |
| `GCP_DATASET_ID` | ID do dataset BigQuery | `promocoes_teste` |

---

## ğŸ“Š Exemplo de ExecuÃ§Ã£o

```bash
$ python scripts/bigquery_teste.py

======================================================================
ğŸ—„ï¸  COLETA MULTI-FONTE COM PAGINAÃ‡ÃƒO + BIGQUERY
======================================================================

ğŸ“‹ CONFIGURAÃ‡ÃƒO:
   Fontes: ['monitor gamer 144hz', 'iphone 16', 'ps5']
   Limite por fonte: 100
   MÃ¡x. pÃ¡ginas por fonte: 3

ğŸ“¦ monitor gamer 144hz
   Produtos coletados: 100
   Em promoÃ§Ã£o: 22
   PreÃ§o mÃ©dio: R$ 1383.64

ğŸ“¦ iphone 16
   Produtos coletados: 100
   Em promoÃ§Ã£o: 45
   PreÃ§o mÃ©dio: R$ 6862.39

ğŸ“¦ ps5
   Produtos coletados: 100
   Em promoÃ§Ã£o: 17
   PreÃ§o mÃ©dio: R$ 5087.34

ğŸ¯ TOTAL COLETADO: 300 produtos

ğŸ“Š RESULTADO DA INSERÃ‡ÃƒO:
   âœ… Inseridos:   292
   â­ï¸  Duplicados:  8
   âŒ Erros:       0

ğŸ“ˆ ESTATÃSTICAS DO BIGQUERY:
   Total de produtos:    302
   Itens Ãºnicos:         298
   Produtos em promoÃ§Ã£o: 84
```

---

## ğŸ“ PrÃ³ximos Passos

- [x] Web scraping com requests + BeautifulSoup
- [x] NormalizaÃ§Ã£o completa (todos os campos do desafio)
- [x] PersistÃªncia no BigQuery
- [x] DeduplicaÃ§Ã£o por `dedupe_key`
- [x] Coleta multi-fonte com paginaÃ§Ã£o
- [ ] API FastAPI com endpoint `/health`
- [ ] Dockerfile e docker-compose
- [ ] Deploy no Cloud Run (GCP)
- [ ] Logs estruturados (JSON)

---

## ğŸ› ï¸ Tecnologias Utilizadas

| Tecnologia | Uso |
|------------|-----|
| Python 3.12 | Linguagem principal |
| requests | RequisiÃ§Ãµes HTTP |
| BeautifulSoup4 | Parsing de HTML |
| Pydantic | ValidaÃ§Ã£o e schemas |
| tenacity | Retry com backoff |
| google-cloud-bigquery | PersistÃªncia no BigQuery |

---

## ğŸ“œ Changelog

### v0.3.0 - Coleta Multi-Fonte com PaginaÃ§Ã£o
- Implementado `fetch_from_sources()` para coleta de mÃºltiplas queries
- Implementado `fetch_products_paginated()` com paginaÃ§Ã£o dinÃ¢mica
- Suporte a 3 fontes simultÃ¢neas: monitor gamer, iphone, ps5
- EstatÃ­sticas de coleta por fonte

### v0.2.0 - IntegraÃ§Ã£o BigQuery
- Criado serviÃ§o `BigQueryService` para persistÃªncia
- Schema da tabela `promotions` com todos os campos do desafio
- **DeduplicaÃ§Ã£o** via verificaÃ§Ã£o prÃ©-inserÃ§Ã£o com `dedupe_key`
- InserÃ§Ã£o via LOAD JOB (compatÃ­vel com free tier GCP)
- MÃ©todos de estatÃ­sticas e busca de produtos recentes

### v0.1.0 - MVP Coleta
- ServiÃ§o de web scraping com BeautifulSoup
- NormalizaÃ§Ã£o de dados com Pydantic
- Retry com backoff exponencial (tenacity)
- ExtraÃ§Ã£o de preÃ§o, desconto, vendedor, imagem

---

## ğŸ“œ Versionamento

Este projeto usa [Semantic Versioning](https://semver.org/). 
Veja o [CHANGELOG.md](CHANGELOG.md) para histÃ³rico completo de mudanÃ§as.

### Como criar uma release

```bash
# Commit suas mudanÃ§as
git add .
git commit -m "feat: nova funcionalidade"

# Crie e push a tag
git tag -a v0.3.0 -m "Release v0.3.0 - Coleta Multi-Fonte"
git push origin v0.3.0
```

O GitHub Actions criarÃ¡ automaticamente a release com os artefatos.

---

> Desenvolvido para o processo seletivo PromoZone